# multi_table_ml_prediction

This project demonstrates a complete end-to-end workflow for integrating siloed datasets, performing data cleaning, engineering, and comparing three supervised learning models (Random Forest, XGBoost, CatBoost).

## ğŸš€ Overview
- Multi-source dataset merging  
- PII handling  
- Missing data imputation  
- Outlier detection  
- Categorical encoding  
- Leakage prevention  
- Full ML pipeline with GridSearchCV  
- Evaluation using Accuracy, F1 score, and confusion matrices  

## ğŸ“ Repository Structure
```
multi-table-ml-prediction/
â”‚
â”œâ”€â”€ README.md
â”œâ”€â”€ data/
â”‚   â”œâ”€â”€ demographics.csv
â”‚   â”œâ”€â”€ behavioral_metrics.csv
â”‚   â””â”€â”€ survey_scores.csv
â”‚
â”œâ”€â”€ notebook/
â”‚   â””â”€â”€ analysis_and_modeling.ipynb
â”‚
â””â”€â”€ src/
    â””â”€â”€ data_prep_utils.py
```

## ğŸ§  Key Skills Demonstrated
- Multi-table merging  
- Data cleaning and hygiene  
- Handling missingness  
- ML pipeline design  
- Hyperparameter tuning  
- Tree-based model comparison  
- Avoiding data leakage  

## ğŸ“Š Modeling Workflow
1. Preprocessing  
2. Train/test split (stratified)  
3. GridSearchCV with 5-fold cross-validation  
4. Model training  
5. Evaluation & comparison  

## ğŸ† Results
Typical results show strong performance across all models, with XGBoost or CatBoost often achieving the best F1 score.

## ğŸ“˜ Notebook
The full cleaned notebook is located at:  
`/notebook/analysis_and_modeling.ipynb`

---

If you have questions about the workflow or want to extend the project, feel free to reach out.
